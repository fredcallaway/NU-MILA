# Conclusion
The computational theory of mind has provided many important insights. While neuroscience explains what brains do, and classical psychology explains what people do, computational theories may serve as the best explanations for what _minds_ do. However, along with new insight, these theories have brought many new methodological challenges, which remain prevalent some 40 years after the first computational psychological theories were being surfaced. One such issue, on which we have focused, is that of levels of representation. An inherent trait of a computation is multiple realizability [@turing50], a trait that inevitably leads to explanation at multiple levels. It is widely acknowledged at this point that these different levels each have value, but there is massive disagreement on how exactly the levels relate. 

We propose that the only way to address these questions is with an earnest attempt[^attempt] to explicitly and formally connect models at different levels of analysis. One such question that is especially interesting and controversial is that of implementation vs. approximation. Do lower level theories implement higher level theories, or do higher level theories approximate lower level theories? The first option suggests a tight fit between levels of analysis, with linking theories systematically reducing a higher level theory into smaller pieces. This is the case for digital computers: the python with which our models are implemented are directly translated to assembly, machine code, logical gates, and finally transistors. In this case, each level of description is equally true, in the sense that they make the same, accurate predictions of how the computer will behave (at least to a very good approximation). Some cognitive scientists believe that brains are similar to computers in this regard [e.g. @fodor88].

[^attempt]: We say "attempt" because the continued failure to formally connect models may itself be indicative that cognition cannot be rigorously divided into functional and modular components.

However, it is possible that minds are fundamentally different from computers. Under this view, functional descriptions of mind are emergent, fuzzy properties of the underlying neural computations [@mcclelland10a], and it may be impossible to systematically compile these abstractions into neural processing. For example, @chalmers92 discusses operations on compositional, distributed representations that transform the structure holistically, arguing that these operations cannot be described in symbolic terms. This does not preclude a higher level description in non-symbolic terms. For example, _harmony_ [@smolensky06] is a high level description of the processing of neural networks that is not symbolic in nature.

Vector Symbolic Architectures may be a useful tool for approaching these issues. While the basic operations are, to a good approximation, implementations of symbolic operators, these could be augmented with additional operations that lack a clear symbolic interpretion. Although the simplistic generalization and composition algorithms we propose have a direct graphical interpretation, these could be replaced by holistic transformations that do not implement symbolic operations. If the functions remain modular and their purpose remains clear, we can maintain a functional description while sacrificing a direct relationship to symbols. Although symbols are powerful tools, they also limit the space of possible transformations; thus, employing such operations could lead to a more powerful system. An interesting aspect of the VectorGraph model is that the input and output from such an operation can still be understood in symbolic terms (as the edge weights to discrete nodes). However, as more of these non-symbolic operations are used, the graphical representation becomes more of an interpretation than an intrinsic trait of the system.

The relationship between levels of analysis is a major open question in the cognitive sciences. It is as of yet unclear how well the functioning of minds can be described by symbolic models, or any modular abstractions for that matter. We have suggested that graphs and Vector Symbolic Architectures are potentially profitable tools for exploring these questions. More broadly, we propose that level-spanning modeling will play an essential role in answering fundamental meta-theoretical questions in cognitive science, and in the construction of a cohesive theory of mind.


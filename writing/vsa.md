# Vector Symbolic Architectures

CHANGE CHANGE

Vector Symbolic Architectures (VSAs) are a class of model that aim to implement a symbolic system with vectors in a very high dimension space [@gayler98; @kanerva88; @plate95]. These models are often called connectionist because they share what Chalmers calls "the deepest philosophical commitment of the connectionist endeavor" [-@chalmers90]: distributed representation of meaning. Like more common connectionist models, (i.e. neural networks), VSA models employ simple distributed representations and linear algebra operations to transform these representations. However, unlike neural networks, computation in VSAs is performed with algebraic operators. In addition to making the computation of VSAs relatively transparent, this gives VSA the power of recursive variable binding [@gayler04], an ability that neural networks have been criticized for lacking [@jackendoff03] [^recursive]. As the name implies, the suggestion is that these models may provide a bridge between symbolic models and the distributed representations that are characteristic of neural processing.

[^recursive]: VSA's are not actually recursive in the standard linguistic sense which implies unbounded recursion. Because all items are stored in vectors of the same size, the amount of interference increases as compositional depth increases, making recursion somewhat fuzzy. We see this as a strength.

VSAs are conceptually descended from the work of Smolensky, who demonstrated that variable binding can be accomplished with the tensor product operation [-@smolensky90]. Here, variable binding refers to the ability to rapidly synthesize novel representations by combining existing representations in a principled way, an ability that many cognitive scientists see as essential for human-like cognition (if not cognition more generally). The critical problem with Smolensky's model is that the tensor product causes dimensionality to increase exponentially as elements are recursively composed, making a large scale implementation infeasible. However, later work found that operations such as circular convolution [@plate95] and pairwise multiplication [@gayler98] serve as reasonable approximations of the tensor product, allowing all items to be represented in the same vector space, regardless of internal compositional structure.

@gayler04 suggests that VSAs all have three fundamental operations that are addition-like, multiplication-like, and permutation-like. However, the function of these operations differ across models, due partially to the fact that permutation and multiplication have similar properties [@kanerva09]. Thus, we supplement Gayler's implementation-focused operator definitions with the following function-focused operators:

#. _bundle_ aggregates vectors into a flat, set-like representation.
#. _label_ tags a content vector with a variable/role vector.
#. _merge_ composes two or more vectors into a structured, tree-like representation.

The bundle operation [terminology from @gayler98] is addition in every VSA we are aware of. This operation is often used to construct _memory vectors_, which store long term knowledge. @plate95 shows how such vectors can be used as an associative memory by labeling content vectors with a vector representing the name/variable for the stored item. The _semantic vectors_ of @jones07 are also memory vectors, as are the _semantic pointers_ of @eliasmith13.

Label and merge have widely varying and overlapping implementations. The term _bind_ is sometimes used to refer to both types of operations; however we separate the terms due to their different functions. Label is the simpler of the two: it always takes two vectors, one which generally serves only as a label (e.g. a variable name), and another which generally represents a more contentful item (e.g. a word). All VSAs of which we are aware implement _label_ as a single multiplicative or permutational operation.

Merge is more complex. It may take any number of arguments, and it can be implemented in numerous ways. Often, it is not a fundamental operation from an implementation view, but rather a composite of addition-like, multiplication-like, and permutation-like operations. For example, because circular convolution is transitive, @plate95 suggests that an ordered bind/merge operation could be implemented by permuting (labeling) the operands before convolving them. @jones07 employs this ordered bind operation in their own merge function, which represents N-grams surrounding a single word by binding the words in a chain, with a special vector representing the target word's location. For example, $\n{a} \circledast \Phi \circledast \n{bit}$ would be added to \n{dog}'s semantic vector, representing the occurrence of "a dog bit". The merge operation can also be implemented by permutation (label) followed by addition (bundle). In this case, the label specifies the structural role of each element in the merge. This strategy maintains the property that the resulting vector is dissimilar from the constituents; however, it differs in that vectors with partially overlapping constituents (e.g. "the dog" and "a dog") will be somewhat similar [@sahlgren08]. @basile11 employ a similar technique; however, they label words by their syntactic dependencies rather than linear position.

The relationship of VSAs to neurons is not fully fleshed out. However, to supplement the intuition that distributed representations are more neurally representative than symbolic ones, we point to the Neural Engineering Framework [@eliasmith03], a neural spiking model that closely reflects neural behavior. These representations have a transparent relationship to vectors, and NEF has been described as a compiler that translates vector operations such as circular convolution into neural spikes [@eliasmith13]. Indeed, @blouw15 present a model of concept learning based on this framework, presenting results computed with a neural spiking model. However, the _semantic pointers_ used in this model are very simple, structurally identical to the associative memories of @plate95. Further work is needed to determine whether structured, compositional representations and operations can be supported by this implementation.


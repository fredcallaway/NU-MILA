{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effect of Chunk Threshold\n",
    "\n",
    "Here we demonstrate the effect (or lack thereof) of chunk threshold on the model's accuracy. We begin by setting up 10 models with varying chunk thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'svg'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/fred/Umila/numila\n"
     ]
    }
   ],
   "source": [
    "cd ~/Umila/numila"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fred/anaconda/lib/python3.5/site-packages/sklearn/utils/fixes.py:64: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  if 'order' in inspect.getargspec(np.copy)[0]:\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import main\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained on 10000 utterances\n",
      "train time : 452.015 seconds\n",
      "num nodes:  11203\n",
      "train time : 71.462 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-b373e1215437>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     model, history = main.train(train_corpus,\n\u001b[1;32m     14\u001b[0m                                 \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                                 \u001b[0mmodel_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                           )\n\u001b[1;32m     17\u001b[0m     \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchunk_thresh\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fred/Umila/numila/main.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(corpus, utterances, track, sample_rate, model_params)\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mutterances\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_utterance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'trained on {} utterances'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fred/Umila/numila/numila.py\u001b[0m in \u001b[0;36mparse_utterance\u001b[0;34m(self, utterance, learn, verbose)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse_utterance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutterance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'Parse'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutterance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mutterance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutterance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fred/Umila/numila/holograph.py\u001b[0m in \u001b[0;36mdecay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     98\u001b[0m         to its initial state\"\"\"\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrow_vec\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid_vec\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DECAY_RATE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# set up and train the models\n",
    "\n",
    "corpus = main.syl_corpus()  # a generator\n",
    "train_corpus = [next(corpus) for _ in range(10000)]\n",
    "test_corpus = [next(corpus) for _ in range(1000)]\n",
    "\n",
    "models = OrderedDict()\n",
    "for chunk_thresh in (i/10 for i in range(1,11)):\n",
    "    params = {'DECAY_RATE': .0001,\n",
    "              'FTP_PREFERENCE': 0.5,\n",
    "              'CHUNK_THRESHOLD': chunk_thresh,\n",
    "              'DIM': 1000}\n",
    "    model, history = main.train(train_corpus,\n",
    "                                track=None,\n",
    "                                model_params=params,\n",
    "                          )\n",
    "    models[chunk_thresh] = model\n",
    "    print('num nodes: ', len(model.graph.nodes))\n",
    "\n",
    "# As a baseline, we use an untrained model.\n",
    "import numila\n",
    "untrained = numila.Numila()\n",
    "models['baseline'] = untrained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the models\n",
    "\n",
    "To test the model, we use a bag of words production task. The model receives a scrambled version of a novel utterance from the corpus and attempts to order it. See `Numila.speak()` for details.\n",
    "\n",
    "We evaluate the models' performance using two metrics, as defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def exactly_equal_metric(lst1, lst2):\n",
    "    \"\"\"1 if the lists are the same, otherwise 0\"\"\"\n",
    "    return 1 if lst1 == lst2 else 0\n",
    "\n",
    "def common_neighbor_metric(lst1, lst2):\n",
    "    \"\"\"The percentage of adjacent pairs that are shared in two lists.\n",
    "    Note that the metric is sensitive to the number of times a given\n",
    "    pair occurs in each list.\n",
    "    \n",
    "    [1,2,3] [3,1,2] -> 0.5\n",
    "    [1,2,3,1,2], [1,2,2,3,1] -> 0.75\n",
    "    \"\"\"\n",
    "    pairs1 = Counter(utils.neighbors(lst1))\n",
    "    pairs2 = Counter(utils.neighbors(lst2))\n",
    "    num_shared = sum((pairs1 & pairs2).values())\n",
    "    possible = sum(pairs1.values())\n",
    "    return num_shared / possible\n",
    "\n",
    "def evaluate_model(model, test_corpus, metric_func):\n",
    "    \"\"\"Evaluates a model's performance on a test corpus based on a given metric.\n",
    "    \n",
    "    metric_func takes in two lists and returns a number between 0 and 1\n",
    "    quantifying the similarity between the two lists.\n",
    "\n",
    "    Returns a list of metric scores comparing an adult utterance to\n",
    "    the model's reconstruction of that utterance from a\n",
    "    scrambeled \"bag of words\" version of the utterance.\n",
    "    \"\"\"\n",
    "    # TODO: break down scores by list length\n",
    "    scores = []\n",
    "    for adult_utt in test_corpus:\n",
    "        if len(adult_utt) > 1:  # can't evaluate a one word utterance\n",
    "            parse = model.speak(adult_utt)\n",
    "            model_utt = utils.flatten_parse(parse)\n",
    "            scores.append(metric_func(model_utt, adult_utt))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate each model using the two metrics.\n",
    "common_neighbor = OrderedDict()\n",
    "exactly_equal = OrderedDict()\n",
    "\n",
    "for chunk_thresh, model in models.items():\n",
    "    scores = evaluate_model(model, test_corpus, common_neighbor_metric)\n",
    "    common_neighbor[chunk_thresh] = scores\n",
    "    \n",
    "    scores = evaluate_model(model, test_corpus, exactly_equal_metric)\n",
    "    exactly_equal[chunk_thresh] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(common_neighbor)\n",
    "mdf = pd.melt(df, var_name='chunk_threshold', value_name='common_neighbor_metric')\n",
    "sns.barplot('chunk_threshold', 'common_neighbor_metric', data=mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(exactly_equal)\n",
    "mdf = pd.melt(df, var_name='chunk_threshold', value_name='exactly_equal_metric')\n",
    "sns.barplot('chunk_threshold', 'exactly_equal_metric', data=mdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We find that chunking has no significant effect on the model's performance on the bag of words production task. This is true even when the threshold is set to 1.0, eliminating all chunks. On the bright side, the model does consistently perform above baseline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
